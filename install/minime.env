# MiniMe-MCP Environment Configuration
# Copy this file to .env and customize your settings
# The Makefile will automatically load these settings

# ============================================================================
# Timezone Configuration (Optional)
# ============================================================================
# Set container timezone to sync with host time for accurate job scheduling
# Examples: America/New_York, Europe/London, Asia/Tokyo, UTC
# Leave commented to auto-detect or default to UTC
TZ=America/New_York

# ============================================================================
# LLM Configuration
# ============================================================================

# Model name for generating insights and analysis
# Used by: llm-service.js, sub-cluster-processor.js, llm-category-processor.js
# Examples: 
#   Ollama: qwen2.5-coder:7b, llama3.2:3b, mistral:7b, gpt-oss:20b
#   OpenAI: gpt-4-turbo, gpt-4o, gpt-3.5-turbo
#   Anthropic: claude-3-opus, claude-3-sonnet
LLM_MODEL=qwen2.5-coder:7b

# LLM Provider determines API endpoints and processing behavior
# Used by: llm-service.js, batch-size-manager.js
# Options: ollama (local), openai, anthropic, azure
# Impact: Affects batch processing (ollama=sequential, others=parallel)
LLM_PROVIDER=ollama

# Context window size in tokens - CRITICAL for clustering configuration
# Used by: context-aware-sizer.js, recursive-clustering-processor.js
# Impact: Determines clustering depth and memory grouping sizes
#   4096: Ultra-deep clustering (5-6 levels), 3-5 memories per cluster
#   16384: Deep clustering (3-4 levels), 10-15 memories per cluster  
#   32768: Moderate clustering (2-3 levels), 20-30 memories per cluster
#   65536: Shallow clustering (1-2 levels), 30-50 memories per cluster
#   131072: Minimal clustering (1 level), 50+ memories per cluster
LLM_CONTEXT_WINDOW=32768

# Sub-cluster processing batch size (how many sub-clusters to process at once)
# Used by: batch-size-manager.js, v2-async-processor.js
# 0 = Auto-detect based on provider (ollama=1, openai=5, anthropic=3, azure=10)
# Override only if you want to force sequential (1) or parallel processing
SUB_CLUSTER_BATCH_SIZE=1

# API Keys (only needed for cloud providers)
OPENAI_API_KEY=
ANTHROPIC_API_KEY=
AZURE_API_KEY=
AZURE_ENDPOINT=

# ============================================================================
# Container Configuration
# ============================================================================
# Container and image names
CONTAINER_NAME=minimemcp
IMAGE_NAME=minimemcp
# VERSION is managed in ../VERSION file and read by Makefile (don't set here)

# ============================================================================
# Host Port Mappings (External Access)
# ============================================================================
# These control how ports are exposed on YOUR HOST MACHINE for accessing the container
# Change these if you have port conflicts on your machine
#
# Example: If port 9000 is already in use, change HOST_UI_PORT=9001
#          Then access UI at http://localhost:9001 (maps to container's internal port 9000)
#
# IMPORTANT: Container internal ports are FIXED and cannot be changed:
#   - MCP Server: 8000 (internal)
#   - UI Server: 9000 (internal)
#   - PostgreSQL: 5432 (internal)
#   - Debug Port: 9229 (internal)
#   - Tag Service: 5001 (internal only, not exposed)

HOST_MCP_PORT=8000
HOST_UI_PORT=9000
HOST_POSTGRES_PORT=5432
HOST_DEBUG_PORT=9229

# Ollama Configuration (runs on host machine, not in container)
OLLAMA_PORT=11434

# ============================================================================
# Processing Configuration
# ============================================================================

# Process memories immediately when added vs queuing for batch processing
# Used by: unified-insights-service.js, v2-async-processor.js
# true: Analyze and cluster memories as soon as they're stored (immediate insights)
# false: Queue memories for later batch processing (better for bulk imports)
# Ollama Recommendation: true (local models respond quickly, no rate limits)
REAL_TIME_PROCESSING=true

# Number of memories to process together in a single batch
# Used by: v2-async-processor.js, clustering-processor-v2.js
# Impact: Lower (5-10) = less memory usage, more API calls
#         Higher (20-50) = more memory usage, fewer API calls, faster overall
# Note: Different from SUB_CLUSTER_BATCH_SIZE which controls sub-cluster processing
# Ollama Recommendation: 5-10 (smaller batches work better with single-threaded processing)
BATCH_SIZE=10

# Maximum parallel operations (embeddings, clustering, insights)
# Used by: v2-async-processor.js, unified-insights-service.js
# Examples: 5 memories embedded at once, 5 clusters analyzed in parallel
# Tuning: Increase for cloud APIs (10-20), decrease for local models (2-5)
# Ollama Recommendation: 2-3 (Ollama handles one request at a time, too many parallel = queue buildup)
MAX_CONCURRENT=3

# Enable background job queue for non-blocking processing
# Used by: unified-insights-service.js, service-initializer.js
# true: Processing happens asynchronously in background (recommended)
# false: Processing blocks until complete (only for debugging)
# Ollama Recommendation: true (keeps UI responsive while processing)
ENABLE_ASYNC_QUEUE=true

# Number of background workers processing the job queue
# Used by: v2-async-processor.js
# Impact: How many jobs run in parallel
# Recommendations: 1 for Ollama, 2-3 for cloud APIs, 4+ for high-volume
QUEUE_WORKERS=1

# ============================================================================
# Feature Flags
# ============================================================================

# Enable pattern detection across memories (code patterns, solution patterns)
# Used by: unified-insights-service.js, pattern-extraction-processor.js
# Impact: Identifies recurring coding patterns and architectural decisions
# Disable to skip pattern analysis and reduce processing time
ENABLE_PATTERN_MATCHING=true

# Enable automatic discovery of relationships between memories
# Used by: unified-insights-service.js, relationship-processor.js
# Impact: Links related memories, finds dependencies, tracks evolution
# Disable to skip relationship discovery and speed up processing
ENABLE_RELATIONSHIP_FINDING=true

# Extract technology stack and preferences from memories
# Used by: unified-insights-service.js, technology-extraction-processor.js
# Impact: Tracks languages, frameworks, libraries used across projects
# Disable to skip technology analysis
ENABLE_TECHNOLOGY_EXTRACTION=true

# Master switch for unified insights processing pipeline
# Used by: service-initializer.js, unified-insights-service.js
# Impact: Controls entire insights system (clustering, analysis, patterns)
# false: Memories stored but not analyzed (fastest, no insights)
# true: Full processing pipeline activated
ENABLE_UNIFIED_INSIGHTS=true

# Insights processing version (v1=legacy, v2=enhanced with clustering)
# Used by: service-initializer.js, routes/insights.js
# Impact: v2 includes recursive clustering, better LLM integration
# v1: Simple insights without clustering (deprecated)
# v2: Advanced clustering with context-aware processing (recommended)
INSIGHTS_VERSION=v2

# ============================================================================
# Data Configuration
# ============================================================================

# Load sample memories and projects on first startup
# Used by: docker-entrypoint.sh, sql-bootstrap/MINIME_DATA_INSERT_*.sql
# Impact: Populates database with example data for testing
# true: Loads ~100 sample memories across 3 projects (first run only)
# false: Start with empty database
LOAD_SAMPLE_DATA=true

# Download and configure local Ollama models during startup
# Used by: docker-entrypoint-with-local-models.sh, prepare-local-models.sh
# Impact: Downloads embedding model (mxbai-embed-large) and LLM model
# true: Auto-download models if not present (~500MB-2GB download)
# false: Assume models already available in Ollama
USE_LOCAL_MODELS=true

# ============================================================================
# Database Configuration
# ============================================================================

# PostgreSQL database credentials
# Used by: docker-compose.yml, knexfile.js, docker-entrypoint.sh
# Impact: All components use these to connect to the database
# Security: Change these in production environments!
POSTGRES_USER=minime
POSTGRES_PASSWORD=minime_password

# Database name for storing memories and insights
# Used by: knexfile.js, sql-bootstrap scripts
# Impact: All tables created in this database
# Note: Changing requires rebuilding container or manual migration
POSTGRES_DB=minime_memories

# PostgreSQL data directory path inside container
# Used by: docker-entrypoint.sh for pg_ctl and initdb
# Impact: Where PostgreSQL stores all database files
# Default: /data/postgres (mapped to Docker volume)
# Note: Changing requires volume remapping and data migration
PGDATA=/data/postgres

# ============================================================================
# Ollama Configuration
# ============================================================================

# ============================================================================
# Embedding Provider Configuration
# ============================================================================
# MiniMe supports multiple embedding providers:
#   1. ONNX (local, offline, no external dependencies) - RECOMMENDED
#   2. Remote API (OpenAI, Azure, custom endpoints)
#   3. Ollama (local service, deprecated - use ONNX instead)
#
# The default embedding model is configured in the database (embedding_models table)
# This section configures provider-specific settings

# --- ONNX Provider (Local, Offline) ---
# Model: Qwen3-Embedding-0.6B with q8 quantization (~800MB)
# Performance: 50-100ms per embedding on CPU
# Benefits: No external dependencies, fully offline, cost-free
# Used when: model provider='onnx' in database

# ONNX model quantization level
# Options: q8 (recommended, 800MB), fp32 (full precision, 1.2GB), q4 (smaller, lower quality)
# Impact: Trade-off between model size, speed, and quality
EMBEDDING_DTYPE=q8

# ONNX computation device
# Options: cpu (default), gpu (if available)
# Impact: GPU provides 2-5x faster embeddings if supported
EMBEDDING_DEVICE=cpu

# HuggingFace model cache directory
# Used by: ONNX provider to store downloaded models
# Impact: Models are pre-cached during Docker build for offline operation
# Default: /app/.cache/huggingface (inside container)
HF_CACHE_DIR=/app/.cache/huggingface

# ONNX model initialization timeout (milliseconds)
# Used during: Initial model loading (first startup only)
# Impact: Model loading can take 30-60 seconds on first run
# Default: 120000 (2 minutes)
EMBEDDING_TIMEOUT=120000

# --- Remote API Provider (OpenAI, Azure, Custom) ---
# Used when: model provider='openai', 'azure', or 'remote' in database
# Benefits: High quality, latest models, no local resources needed
# Costs: API fees per request

# OpenAI API configuration
# Used by: RemoteEmbeddingProvider when provider='openai'
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# OpenAI base URL (optional, for custom deployments)
# Default: https://api.openai.com/v1
# Override for: Azure OpenAI, custom endpoints, proxies
# OPENAI_BASE_URL=https://api.openai.com/v1

# Azure OpenAI configuration (optional)
# Used by: RemoteEmbeddingProvider when provider='azure'
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=

# Generic API provider configuration (for custom endpoints)
# Used by: RemoteEmbeddingProvider for non-OpenAI/Azure providers
# EMBEDDING_API_KEY=your-custom-api-key
# EMBEDDING_API_BASE_URL=https://your-custom-endpoint.com/v1

# Remote API timeout (milliseconds)
# Default: 30000 (30 seconds)
EMBEDDING_API_TIMEOUT=30000

# Remote API retry attempts
# Default: 3 (retry failed requests up to 3 times)
EMBEDDING_API_RETRIES=3

# --- Ollama Configuration (DEPRECATED - Use ONNX instead) ---
# Ollama service URL (for LLM only - embeddings use ONNX)
# Used by: llm-service.js, docker-entrypoint.sh
# Impact: Where to send LLM requests (insights generation)
# Options:
#   Docker Desktop/Podman: http://host.docker.internal:11434 (recommended)
#   Linux host: http://172.17.0.1:11434
#   Remote Ollama: http://your-ollama-server:11434
OLLAMA_HOST=http://host.docker.internal:11434

# ============================================================================
# Document Upload & Embedding Configuration
# ============================================================================

# Document chunk size in characters (deprecated - use CHUNK_SIZE_TOKENS instead)
# Used by: chunking-service.js (RecursiveCharacterTextSplitter)
# Impact: Controls maximum size of document chunks for embedding
# Formula: chars / 3.5 ≈ tokens (approximate, varies by tokenizer)
# Constraint: Must stay under embedding model's context window (512 tokens for mxbai-embed-large)
# Recommendations:
#   1500 chars (~430 tokens): Safe for BERT-based models with 512-token limit (recommended)
#   1750 chars (~500 tokens): Tighter margin, may fail on special characters
#   3500 chars (~1000 tokens): Requires models with larger context windows
# Note: RecursiveCharacterTextSplitter respects semantic boundaries (paragraphs/sentences)
# Default: 1500 (safe for mxbai-embed-large with safety margin for tokenization differences)
CHUNK_SIZE_CHARS=1500

# Document chunk size in TOKENS (RECOMMENDED - uses BERT tokenizer for accuracy)
# Used by: chunking-service.js with BERT token counting
# Impact: Controls maximum tokens per chunk using exact BERT tokenization
# Constraint: MUST NOT exceed embedding model's context window
# mxbai-embed-large limit: 512 tokens (PARAMETER num_ctx 512)
# Recommendations:
#   400 tokens: Safe with 22% margin for tokenizer differences (recommended)
#   450 tokens: Moderate margin - may still fail on some chunks
#   500 tokens: UNSAFE - BERT tokenizer ≠ mxbai tokenizer, causes failures
#   512 tokens: Maximum - will definitely fail
# CRITICAL: BERT tokenizer counts differently than mxbai's internal tokenizer!
CHUNK_SIZE_TOKENS=400

# Document chunk overlap in characters (deprecated - use CHUNK_OVERLAP_TOKENS instead)
# Used by: chunking-service.js (RecursiveCharacterTextSplitter)
# Impact: How much context is shared between adjacent chunks
# Purpose: Ensures semantic continuity and prevents information loss at chunk boundaries
# Recommendations:
#   10-15% of CHUNK_SIZE_CHARS: Good balance (175 chars for 1500, 200 chars for 1750)
#   Higher overlap: Better context but more redundancy and storage
#   Lower overlap: Less redundancy but risk of losing context
# Default: 175 (11.6% overlap, good for maintaining context)
CHUNK_OVERLAP_CHARS=175

# Document chunk overlap in TOKENS (RECOMMENDED)
# Used by: chunking-service.js with BERT token counting
# Impact: How many tokens overlap between adjacent chunks
# Recommendations:
#   50 tokens: ~10% of 500 token chunks (good balance)
#   64 tokens: ~12.5% overlap (slightly more context)
CHUNK_OVERLAP_TOKENS=50

# Number of parallel threads for document chunk embedding
# Used by: document-service.js (insertChunksWithEmbeddings)
# Impact: Controls concurrent embedding generation during document upload
# Recommendations:
#   Ollama (single-threaded): 1 (sequential processing, no timeouts)
#   OpenAI/Anthropic: 20-30 (high parallelism for cloud APIs)
#   Powerful local Ollama: 2-3 (if hardware can handle multiple requests)
# Default: 1 (safe for all setups)
EMBEDDING_CONCURRENT_THREAD_POOL=1

# Maximum retry attempts for failed embedding requests
# Used by: document-service.js (generateEmbeddingWithRetry)
# Impact: How many times to retry before giving up on a chunk
# 0: No retries (fail immediately)
# 3: Retry up to 3 times (default, recommended for reliability)
# 5+: Very persistent (good for unstable connections)
EMBEDDING_MAX_RETRIES=3

# Base delay in milliseconds for exponential backoff retries
# Used by: document-service.js (generateEmbeddingWithRetry)
# Impact: Wait time between retry attempts (doubles each retry)
# Formula: delay = baseDelay * 2^(attempt-1)
# Examples with 1000ms:
#   Retry 1: 1 second
#   Retry 2: 2 seconds
#   Retry 3: 4 seconds
# Default: 1000 (1 second base delay)
EMBEDDING_RETRY_DELAY_MS=1000

# ============================================================================
# Tag Generation Service Configuration
# ============================================================================

# Python FastAPI tag service port (internal only)
# Used by: docker-entrypoint.sh, document-service.js
# Impact: Port where YAKE keyword extraction service runs
# Default: 5001 (internal only, not exposed outside container)
TAG_SERVICE_PORT=5001

# Tag service internal URL
# Used by: document-service.js for chunk tagging
# Impact: Where to send document chunks for keyword extraction
# Note: Should match TAG_SERVICE_PORT above
TAG_SERVICE_URL=http://localhost:5001

# Maximum retry attempts for tag service requests
# Used by: document-service.js when tag service is unavailable
# Impact: How many times to retry failed tagging requests
# Default: 3 (good balance between reliability and timeout)
TAG_SERVICE_MAX_RETRIES=3

# Tag service request timeout in milliseconds
# Used by: document-service.js for HTTP requests to tag service
# Impact: How long to wait before considering tag request failed
# Default: 30000 (30 seconds, allows for large documents)
TAG_SERVICE_TIMEOUT=30000

# ============================================================================
# User Configuration
# ============================================================================

# Default user ID for single-user mode
# Used by: memory creation, document uploads, thinking sequences
# Impact: All content is attributed to this user
# Future: Will be replaced with OAuth/SSO user ID in multi-user mode
# Default: 'system' (single-user mode)
DEFAULT_USER_ID=system

# ============================================================================
# Logging Configuration
# ============================================================================
# Log level: debug, info, warn, error
LOG_LEVEL=info


# ============================================================================
# Runtime Environment Configuration
# ============================================================================

# Node.js environment mode
# Used by: All Node.js services (MCP server, UI server, docker-entrypoint.sh)
# Impact: Enables/disables development features and logging
# Options: development, production
# Default: production (optimized for performance)
NODE_ENV=production

# ============================================================================
# Debug Configuration
# ============================================================================

# Enable detailed MCP protocol debugging
# Used by: mcp-server.js, mcp-tools-v3.js
# Impact: Logs all MCP messages, tool calls, and responses
# true: Verbose logging for troubleshooting MCP issues
# false: Normal logging only (recommended for production)
MCP_DEBUG=false

# Node.js heap memory limit in MB
# Used by: docker-entrypoint.sh, node process startup
# Impact: Maximum memory Node.js can use for processing
# 2048: 2GB heap (good for 500-1000 memories)
# 4096: 4GB heap (good for 1000-5000 memories)
# 8192: 8GB heap (good for 5000+ memories or large clusters)
# Increase if you see "JavaScript heap out of memory" errors
NODE_MAX_OLD_SPACE_SIZE=8192

# ============================================================================
# Volume Names
# ============================================================================

# Docker volume for persistent data (PostgreSQL, Ollama models)
# Used by: Makefile (init-volume, clean-volume targets), docker-compose.yml
# Impact: All persistent data stored here
# Note: Data persists across container restarts
# Warning: 'make clean-all' will delete this volume and all data!
#
# Volume naming convention (automatically constructed):
#   Main data volume: ${VOLUME_NAME}
#   PostgreSQL WAL:   ${VOLUME_NAME}-wal
#   Documents:        ${VOLUME_NAME}-docs
# Example: If VOLUME_NAME=minime-mcp-v18, then volumes are:
#   - minime-mcp-v18 (main data)
#   - minime-mcp-v18-wal (PostgreSQL Write-Ahead Log)
#   - minime-mcp-v18-docs (uploaded documents)
VOLUME_NAME=minime-mcp-v18
